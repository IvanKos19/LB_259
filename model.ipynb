# model.ipynb

# 1. Bibliotheken importieren
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 2. Datensatz laden
df = pd.read_csv('synthetic_beverage_sales_data.csv')

# 3. Input-Features und Zielvariable definieren
X = df.drop(columns=["Region"])  # Alle Spalten außer "Region" sind Eingaben
y = df["Region"]                 # Ziel: Region vorhersagen

# 4. Datensatz aufteilen in Training und Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Modell erstellen und trainieren
model = RandomForestClassifier()
model.fit(X_train, y_train)

# 6. Vorhersagen machen
y_pred = model.predict(X_test)

# 7. Ergebnisse vergleichen
vergleich = pd.DataFrame({
    "Echter Wert": y_test.values,
    "Vorhergesagt": y_pred
})

vergleich.head(10)  # Zeige die ersten 10 Vorhersagen

# 8. Modell bewerten
print(classification_report(y_test, y_pred))

# 9. Begründung (als Markdown-Zelle im Notebook)
# Begründung (50-100 Wörter):
"""
Ich habe den Random Forest Classifier gewählt, weil er sehr gut bei Klassifikationsproblemen funktioniert, 
auch wenn viele Features beteiligt sind. Random Forest verhindert Überanpassung (Overfitting) durch 
den Einsatz vieler Entscheidungsbäume und erzielt oft robuste Vorhersagen. 
Er eignet sich gut für heterogene Datensätze wie Getränkekäufe, wo viele Einflüsse bestehen.
"""

# 10. Erkenntnisse aus Vorhersagen (als Markdown-Zelle im Notebook)
# Erkenntnisse (50-100 Wörter):
"""
Das Modell kann die Regionen meist korrekt vorhersagen. 
Einige Abweichungen sind nachvollziehbar, da ähnliche Produkte in benachbarten Regionen verkauft werden könnten. 
Insgesamt erscheint das Modell sinnvoll und geeignet für erste Analysen. 
Eine weitere Verbesserung wäre möglich durch Feature-Engineering oder komplexere Modelle.
"""
